---
title: "Week13_Warmup"
author: "Vivian Zeng, U of Notre Dame"
date: "11/10/2020"
output: html_document
---
```{r}
library(stm)
```

```{r}
readLines("week15-practice.csv")[1:10]
```
Read the data
```{r}
week13 = read.csv("week15-practice.csv", stringsAsFactors = FALSE)
```

Pree-processing 
```{r}
week13Text = textProcessor(documents = week13$review, 
                           metadata = week13)
```

Remove non-graphical characters within the data, and remove that offending character:
```{r}
rvest::guess_encoding(week13$review)
```

```{r}
week13$review = iconv(week13$review, "ISO-8859-1", "UTF-8", sub = "")
week13$review = gsub("[^[:graph:]]|Ã", " ", week13$review, perl = TRUE)
week13$review = iconv(week13$review, "ISO-8859-1", "UTF-8", sub = "")
```


Take a look at the word "reacted".
```{r}
nrow(week13[grepl("REDACTED", week13$review), ])
```

Add "REACTED" as a custom stopword to our processor. 
```{r}
week13TextProcess = textProcessor(documents = week13$review, 
                           metadata = week13, 
                           onlycharacter = TRUE,
                           customstopwords = c("redacted", 
                                               tm::stopwords("SMART"), 
                                               tm::stopwords("en")))
```


```{r}
week13TextPrep = prepDocuments(documents = week13TextProcess$documents, 
                               vocab = week13TextProcess$vocab,
                               meta = week13TextProcess$meta)
```


```{r}
topic3 = stm(documents = week13TextPrep$documents, 
             vocab = week13TextPrep$vocab, 
             K = 3)
```

Visualize the expected topic proportions for each of the topics and the highest probability words.
```{r}
plot(topic3)
```
The expected topic proportions are pretty even amonst these three, but topic 3 certainly would have the highest expected proportions.

Take a look at the various top words within a topic:
```{r}
labelTopics(topic3)
```
Topic 1 is talking about “being a great place to work”;
Topic 2 is about “opportunities and compensation”
Topic 3 is about “people”. Truly an area where science is more art than science.

Get some exemplar texts for each topic:
```{r}
findThoughts(topic3, texts = week13$review, n = 1)
```
See if we have an “adequate” number of topics.

```{r}
checkResiduals(topic3, documents = week13TextPrep$documents)
```

A significant test statistic here means that we do not have an adequate number of topics – we want our dispersion to be very close to 1. 

Try a 4 topic model:
```{r}
topic4 = stm(documents = week13TextPrep$documents, 
             vocab = week13TextPrep$vocab, 
             K = 4)
```


```{r}
plot(topic4)
labelTopics(topic4)
```

See the differences from topic 3
```{r}
findThoughts(topic4, texts = week13$review, n = 1)
checkResiduals(topic4, documents = week13TextPrep$documents)
```

Our dispersion definitely dropped down just by adding 1 topic, but let’s continue.
```{r}
topic5 = stm(documents = week13TextPrep$documents, 
             vocab = week13TextPrep$vocab, 
             K = 5)
```

```{r}
plot(topic5)
```

```{r}
findThoughts(topic5, texts = week13$review, n = 1)
checkResiduals(topic5, documents = week13TextPrep$documents)
```

Use automatic work with our K selection:

```{r}
kTest = searchK(documents = week13TextPrep$documents, 
             vocab = week13TextPrep$vocab, 
             K = c(3, 4, 5, 10, 20, 25, 35))
```

```{r}
plot(kTest)
```

The 4 plots that are returned are going to try to help us determine the best number of topics to take. We want to have low residual and high semantic coherence. The residuals definitely take a sharp dive as we increase K, but our coherence bounces around some (but appears to spike back up for 35). 

See what 35 looks like
```{r}
topic35 = stm(documents = week13TextPrep$documents, 
             vocab = week13TextPrep$vocab, 
             K = 35)
```


```{r}
plot(topic35)
```


```{r}
labelTopics(topic35)
checkResiduals(topic35, documents = week13TextPrep$documents)
```
There is not even enough topics to have great model fit.

